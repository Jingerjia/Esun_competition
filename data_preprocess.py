#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dataloader.py
è³‡æ–™å‰è™•ç†èˆ‡ JSON è¼¸å‡º
"""

import os
import json
import time
import math
import random
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import argparse

# ========= CONFIG =========
GLOBAL_AMT_MAX = None  # ä¹‹å¾Œç”±çµ±è¨ˆçµæœæˆ–è¨­å®šæª”è®€å…¥
CACHE_DIR = Path("analyze_UI/cache")
DETAILS_DIR = CACHE_DIR / "details"
RANK_DIR = CACHE_DIR / "ranks"
INDEX_JSON = CACHE_DIR / "account_index.json"
DATAFILES_DIR = Path("datafiles")
MAX_MONEY_JSON = DATAFILES_DIR / "max_money.json"
EXCHANGE_JSON = DATAFILES_DIR / "exchange_rate.json"

GLOBAL_CHANNELS = ["PAD", "01", "02", "03", "04", "05", "06", "07", "99", "UNK"]
CHANNEL_CODE = [-1, 1, 2, 3, 4, 5, 6, 7, 8, 0]
CHANNEL_MAP = {c: i for i, c in zip(CHANNEL_CODE, GLOBAL_CHANNELS)}

def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

# ========= UTILS =========
def load_rank_csv(path):
    df = pd.read_csv(path)
    return set(df['acct'].astype(str).tolist())

def piecewise_norm(val_twd):
    # ç·šæ€§ç¸®æ”¾
    thresholds = [100, 1_000, 10_000, 100_000, 1_000_000, 10_000_000, 100_000_000]
    scales =     [0.05, 0.25, 0.45, 0.65, 0.85, 0.95, 1.0]
    if val_twd <= thresholds[0]:
        return (val_twd / thresholds[0]) * scales[0]
    for i in range(1, len(thresholds)):
        if val_twd < thresholds[i]:
            r = (val_twd - thresholds[i-1]) / (thresholds[i] - thresholds[i-1])  
            return scales[i-1] + r * (scales[i] - scales[i-1])
    return 1.0
# --- å®šç¾©äº¤æ˜“ç­†æ•¸ bucket ---
def bucket_txn_count(n):
    if n == 1: return "b1"
    elif n == 2: return "b2"
    elif 3 <= n <= 5: return "b3_5"
    elif 6 <= n <= 10: return "b6_10"
    elif 11 <= n <= 20: return "b11_20"
    elif 21 <= n <= 50: return "b21_50"
    elif 51 <= n <= 100: return "b51_100"
    elif 101 <= n <= 500: return "b101_500"
    else: return "b500p"

def flatten_tokens(args, dataset, alert_accts, mode="train", soft_label=0.3):
    """
    å°‡å¸³æˆ¶ç´šåˆ¥è³‡æ–™è½‰ç‚º (N, 50, 10) tokens
    """
    tokens, masks, labels, accts = [], [], [], []
    for r in dataset:
        if args.one_token_per_day:
            N = len(r["delta_days"])
            tok = []
            for i in range(N):
                tok.append([
                    r["delta_days"][i],         # 1  è·é›¢ä¸Šæ¬¡äº¤æ˜“å¤©æ•¸
                    r["txn_amt_max"][i],        # 1  ç•¶æ—¥æœ€å¤§äº¤æ˜“é‡‘é¡
                    r["txn_amt_min"][i],        # 1  ç•¶æ—¥æœ€å°äº¤æ˜“é‡‘é¡
                    r["txn_amt_avg"][i],        # 1  ç•¶æ—¥å¹³å‡äº¤æ˜“é‡‘é¡
                    r["txn_count"][i],          # 1  ç•¶æ—¥äº¤æ˜“æ•¸é‡
                    r["txn_count_out"][i],      # 1  åŒ¯æ¬¾äº¤æ˜“æ•¸é‡
                    r["txn_count_in"][i],       # 1  æ”¶æ¬¾äº¤æ˜“æ•¸é‡
                    r["unique_accounts"][i],    # 1  ç•¶æ—¥äº¤æ˜“å¸³è™Ÿæ•¸é‡
                ])
        else:
            # æ¯ç­†è³‡æ–™éƒ½æ˜¯å¸³æˆ¶åºåˆ—
            N = len(r["txn_type"])  # é æœŸ50
            tok = []
            for i in range(N):
                sin_val, cos_val = r["time2vec"][i]
                tok.append([
                    sin_val, cos_val,                   # 2 ç¶­ äº¤æ˜“æ™‚é–“
                    r["day_pos"][i],                    # 1    äº¤æ˜“å¤©æ•¸ (èˆ‡ç•¶å‰æ‰€æœ‰äº¤æ˜“ç›¸æ¯”)
                    r["txn_type"][i],                   # 1    äº¤æ˜“å‹åˆ¥ (æ”¶/åŒ¯æ¬¾)
                    r["channel"][i],                    # 1    äº¤æ˜“é€šè·¯
                    r["currency"][i],                   # 1    äº¤æ˜“å¹£åˆ¥
                    r["is_twd"][i],                     # 1    æ˜¯å¦ç‚ºå°å¹£
                    r["amt_norm"][i],                   # 1    é‡‘é¡
                    r["delta_days_value"][i],           # 1    èˆ‡ä¸Šç­†äº¤æ˜“å·®ç•°å¤©æ•¸
                    r["same_person"][i],                # 1    æ˜¯å¦ç‚ºåŒä¸€äºº
                ])
        tokens.append(tok)
        masks.append(r["mask"])
        # æ¨™ç±¤ï¼šè­¦ç¤ºå¸³æˆ¶ç‚º1ï¼Œå…¶é¤˜0
        if mode == "train" and soft_label > 0:
            label = 1 if r["acct"] in alert_accts else soft_label
        else:
            label = 1 if r["acct"] in alert_accts else 0

        labels.append(label)
        accts.append(r["acct"])

    if mode == "train" and soft_label > 0:
        print(f"mode={mode}, soft_label = {soft_label}, use_soft_label")
    else:
        print(f"mode={mode}, soft_label = {soft_label}, without_soft_label")

    return (
        np.array(tokens, dtype=np.float32),
        np.array(masks, dtype=np.int8),
        np.array(labels, dtype=np.float32),
        np.array(accts)
    )

def normalize_money(x, curr_list, exchange_rate_json, default_currency="TWD", mode="piecewise"):
    """
    ä»¥ã€Œåˆ†è²æ¦‚å¿µã€æ¨™æº–åŒ–é‡‘é¡:
      - å°å¹£åŸºæº–ï¼š
          100 â†’ 0.05
          1000 â†’ 0.25
          1è¬ â†’ 0.45
          10è¬ â†’ 0.65
          100è¬ â†’ 0.85
          1000è¬ â†’ 0.95
          1å„„ä»¥ä¸Š â†’ 1.0
      - å…¶ä»–å¹£åˆ¥ï¼šä¾åŒ¯ç‡æ›ç®—æˆå°å¹£å†å¥—åŒè¦å‰‡
    åƒæ•¸:
        x: é‡‘é¡åˆ—è¡¨
        curr_list: å¹£åˆ¥åˆ—è¡¨
        exchange_rate_json: å¹£åˆ¥å°å°å¹£åŒ¯ç‡ dict
        default_currency: é è¨­å¹£åˆ¥ (TWD)
        mode: "smooth" æˆ– "piecewise"
    """

    def smooth_norm(val_twd):
        # å– log
        if val_twd <= 100:
            return 0.05 * (val_twd / 100)
        norm = 0.05 + 0.22 * (math.log10(val_twd / 100)) ** 0.85
        return min(1.0, max(0.0, norm))

    result = []
    for val, cur in zip(x, curr_list):
        rate = exchange_rate_json.get(cur, exchange_rate_json.get(default_currency, 1.0))
        val_twd = val * rate
        if mode == "piecewise":
            norm = piecewise_norm(val_twd)
        else:
            norm = smooth_norm(val_twd)
        result.append(norm)
    return result

def time2vec_scalar(hour, minute):
    # åŸºç¤ Time2Vec (ç°¡åŒ–ç‰ˆ)
    val = hour * 60 + minute
    return [math.sin(val / 1440 * math.pi), math.cos(val / 1440 * math.pi)]

def bucketize(value, bins):
    for i, b in enumerate(bins):
        if value <= b:
            return i
    return len(bins)


def process_account_per_day(args, acct, meta, index_info, global_exchange):
    """å°‡å–®ä¸€å¸³æˆ¶è³‡æ–™è½‰æ›æˆæ¨¡å‹è¼¸å…¥æ ¼å¼ï¼ŒæŒ‰å¤©å½™æ•´äº¤æ˜“ç´€éŒ„"""
    file_path = DETAILS_DIR / index_info['file']
    start, end = index_info['start'], index_info['end']
    df = pd.read_csv(file_path).iloc[start:end].reset_index(drop=True)
    
    # æŒ‰ txn_date åˆ†çµ„ï¼Œä¸¦æ ¹æ“š txn_time æ’åº
    df['txn_time'] = pd.to_datetime(df['txn_time'], format='%H:%M:%S').dt.time

    # æŒ‰ txn_date å’Œ txn_time æ’åº
    df.sort_values(by=['txn_date', 'txn_time'], inplace=True)

    # åŒ¯ç‡è½‰æ›ï¼šå°‡é‡‘é¡è½‰æ›ç‚ºå°å¹£
    def convert_to_twd(row):
        exchange_rate = global_exchange.get(row['currency_type'], 1.0)  # é è¨­ç‚º1.0 (TWD)
        return row['txn_amt'] * exchange_rate

    # è½‰æ›æ‰€æœ‰äº¤æ˜“é‡‘é¡ç‚ºå°å¹£
    df['txn_amt_twd'] = df.apply(convert_to_twd, axis=1)

    # æŒ‰ txn_date åˆ†çµ„ä¸¦è¨ˆç®—æ¯æ—¥ç‰¹å¾µ
    grouped = df.groupby('txn_date')

    results = []
    last_txn_date = None
    for date, group in grouped:
        # è¨ˆç®—ç•¶æ—¥ç‰¹å¾µ
        txn_count = len(group) # ç•¶æ—¥ç¸½äº¤æ˜“æ•¸
        txn_amt_max = group['txn_amt'].max() # ç•¶æ—¥æœ€å¤§äº¤æ˜“é‡‘é¡
        txn_amt_min = group['txn_amt'].min() # ç•¶æ—¥æœ€å°äº¤æ˜“é‡‘é¡
        txn_amt_avg = group['txn_amt'].mean() # ç•¶æ—¥å¹³å‡äº¤æ˜“é‡‘é¡

        # åŒ¯æ¬¾/æ”¶æ¬¾çš„ç­†æ•¸
        txn_count_out = len(group[group['role'] == 'OUT']) #ç•¶æ—¥åŒ¯æ¬¾äº¤æ˜“æ•¸é‡
        txn_count_in = len(group[group['role'] == 'IN']) #ç•¶æ—¥æ”¶æ¬¾äº¤æ˜“æ•¸é‡

        # ç•¶æ—¥äº¤æ˜“å¸³è™Ÿæ•¸é‡
        unique_accounts = len(set(group['from_acct'].tolist() + group['to_acct'].tolist())) - 1 # ç•¶æ—¥äº¤æ˜“å¸³è™Ÿæ•¸é‡(ä¸å«è‡ªå·±)

        # ç•¶æ—¥çš„è·é›¢ä¸Šæ¬¡äº¤æ˜“å¤©æ•¸ï¼ˆå¦‚æœæœ‰å‰ä¸€å¤©äº¤æ˜“ï¼‰
        if last_txn_date is None:
            delta_days = 0  # ç¬¬ä¸€ç­†äº¤æ˜“ï¼Œç„¡å‰ä¸€æ—¥
        else:
            delta_days = (date - last_txn_date)# è·é›¢ä¸Šæ¬¡äº¤æ˜“å¤©æ•¸

        # -------------------------------- æ¨™æº–åŒ– --------------------------------
        # æŒ‰ç…§æä¾›çš„è¦å‰‡æ¨™æº–åŒ– delta_days
        if delta_days == 0:
            delta_days = 0.1  # åŒæ—¥
        elif delta_days == 1:
            delta_days = 0.2
        elif 2 <= delta_days <= 3:
            delta_days = 0.3
        elif 4 <= delta_days <= 7:
            delta_days = 0.4
        elif 8 <= delta_days <= 10:
            delta_days = 0.5
        elif 11 <= delta_days <= 20:
            delta_days = 0.6
        elif 21 <= delta_days <= 40:
            delta_days = 0.7
        elif 41 <= delta_days <= 70:
            delta_days = 0.8
        elif 71 <= delta_days <= 100:
            delta_days = 0.9
        elif delta_days >= 101:
            delta_days = 1.0
        else:
            delta_days = 0.0
        # å°é‡‘é¡é€²è¡Œæ¨™æº–åŒ–
        txn_amt_max = piecewise_norm(txn_amt_max)
        txn_amt_min = piecewise_norm(txn_amt_min)
        txn_amt_avg = piecewise_norm(txn_amt_avg)
        # å°äº¤æ˜“æ•¸é‡é€²è¡Œæ¨™æº–åŒ–
        txn_count = min(txn_count / 50, 1.0)  # ç¸®æ”¾è‡³ 50
        txn_count_out = min(txn_count_out / 50, 1.0)  # ç¸®æ”¾è‡³ 50
        txn_count_in = min(txn_count_in / 50, 1.0)  # ç¸®æ”¾è‡³ 50
        unique_accounts = min(unique_accounts / 50, 1.0)  # ç¸®æ”¾è‡³ 50

        # è¨˜éŒ„ç•¶æ—¥äº¤æ˜“ç‰¹å¾µ
        result = {
            'delta_days': delta_days,
            'txn_amt_max': txn_amt_max,
            'txn_amt_min': txn_amt_min,
            'txn_amt_avg': txn_amt_avg,
            'txn_count': txn_count,
            'txn_count_out': txn_count_out,
            'txn_count_in': txn_count_in,
            'unique_accounts': unique_accounts
        }
        
        results.append(result)
        last_txn_date = date  # æ›´æ–°ä¸Šæ¬¡äº¤æ˜“æ—¥æœŸ

    seq_len = args.seq_len
    pad_len = seq_len - len(results)

    # é å³ padding
    if pad_len > 0:
        pad_token = {
            'delta_days': -1.0,  # ç‰¹åˆ¥æ¨™ç¤º padding
            'txn_amt_max': 0.0,
            'txn_amt_min': 0.0,
            'txn_amt_avg': 0.0,
            'txn_count': 0.0,
            'txn_count_out': 0.0,
            'txn_count_in': 0.0,
            'unique_accounts': 0.0,
        }
        results = [pad_token] * pad_len + results
    elif pad_len < 0:
        results = results[-seq_len:]  # è¶…éå‰‡æˆªæ–·

    # å°æ‡‰çš„ mask
    mask = [1]*len(results) if pad_len <= 0 else [0]*pad_len + [1]*(seq_len - pad_len)

    #print("\n\n\nmask.shape = ", np.array(mask, dtype=np.int8).shape)
    #print("results.shape = ", np.array(results, dtype=np.float32).shape)
    # æº–å‚™åºåˆ—æ ¼å¼è¼¸å‡º
    result = {
        "acct": acct,
        "delta_days": [r['delta_days'] for r in results],
        "txn_amt_max": [r['txn_amt_max'] for r in results],
        "txn_amt_min": [r['txn_amt_min'] for r in results],
        "txn_amt_avg": [r['txn_amt_avg'] for r in results],
        "txn_count": [r['txn_count'] for r in results],
        "txn_count_out": [r['txn_count_out'] for r in results],
        "txn_count_in": [r['txn_count_in'] for r in results],
        "unique_accounts": [r['unique_accounts'] for r in results],
        "mask": mask,
        "seq_len": min(len(results), seq_len)
    }
    return result


def process_account(args, acct, meta, index_info, global_exchange):
    """å°‡å–®ä¸€å¸³æˆ¶è³‡æ–™è½‰æ›æˆæ¨¡å‹è¼¸å…¥æ ¼å¼"""
    file_path = DETAILS_DIR / index_info['file']
    start, end = index_info['start'], index_info['end']
    df = pd.read_csv(file_path).iloc[start:end].reset_index(drop=True)
    # åƒ…å–æœ€å¾Œ 50 ç­†
    df = df.tail(args.seq_len).reset_index(drop=True)

    # å¡«è£œ padding
    pad_len = args.seq_len - len(df)
    if pad_len > 0:
        pad = pd.DataFrame([{
            'txn_amt': 0,
            'currency_type': 'PAD',
            'is_self_txn': 'UNK',
            'channel_type': 'UNK',
            'txn_date': -1,
            'txn_time': '00:00:00',
            'role': 'PAD'
        }] * pad_len)
        df = pd.concat([pad, df], ignore_index=True)

    # ===== Feature Transform =====
    # äº¤æ˜“å‹åˆ¥
    txn_type = df['role'].apply(lambda x: 1 if x == 'OUT' else (0 if x == 'IN' else -1)).tolist()
    # é€šè·¯ embedding index
    channel_idx = df['channel_type'].apply(lambda x: CHANNEL_MAP.get(x, 0)).tolist()
    # å¹£åˆ¥ embedding index
    curr_map = {c: i for i, c in enumerate(sorted(df['currency_type'].unique()))}
    curr_idx = df['currency_type'].apply(lambda x: curr_map.get(x, 0)).tolist()
    # æ˜¯å¦å°å¹£
    is_twd = df['currency_type'].apply(lambda x: 1 if x == 'TWD' else (0 if x != 'PAD' else -1)).tolist()
    # é‡‘é¡
    amt_norm = normalize_money(df['txn_amt'].tolist(),
                                df['currency_type'].tolist(),
                                global_exchange
                                )
    # æ˜¯å¦åŒäºº
    same_person = df['is_self_txn'].apply(lambda x: 1 if x == 'Y' else (0 if x == 'N' else -1)).tolist()
    # ----------------------------- å·®è·å¤©æ•¸ bucket ---------------------------------
    # å…ˆç¢ºä¿ txn_date å·²æ’åº
    #df = df.sort_values('txn_date').reset_index(drop=True)

    days = df['txn_date'].astype(float).tolist()
    #print(f"days = {days}")
    delta_days = []
    for i in range(len(df)):
        if df.loc[i, 'role'] == 'PAD':          # padding token
            delta_days.append(-1)
        elif days[i-1] == -1:
            delta_days.append(0.5)                # ç¬¬ä¸€ç­†
        else:
            d = days[i] - days[i-1]
            #print(f'days[i] = {days[i]}')
            #print(f'days[i-1] = {days[i-1]}')
            if d == 0:
                delta_days.append(0)          # åŒä¸€å¤©äº¤æ˜“
            else:
                delta_days.append(d)
                    
    # ----------------------------- å·®è·å¤©æ•¸ç­‰æ¯”ä¾‹æ˜ å°„ [-1, 1] -----------------------------
    delta_days_value = []
    for diff in delta_days:
        if diff == -1:
            delta_days_value.append(-1.0)
            continue
        if diff == 0.5:
            delta_days_value.append(0.0)   # é¦–ç­†
            continue

        if diff == 0:
            val = 0.1                      # åŒæ—¥
        elif diff == 1:
            val = 0.2
        elif 2 <= diff <= 3:
            val = 0.3
        elif 4 <= diff <= 7:
            val = 0.4
        elif 8 <= diff <= 10:
            val = 0.5
        elif 11 <= diff <= 20:
            val = 0.6
        elif 21 <= diff <= 40:
            val = 0.7
        elif 41 <= diff <= 70:
            val = 0.8
        elif 71 <= diff <= 100:
            val = 0.9
        elif diff >= 101:
            val = 1.0
        else:
            val = 0.0
        delta_days_value.append(val)
    # ----------------------------- å±€éƒ¨ day_position -----------------------------
    # txn_date ç‚ºåˆ‡é½Šç¬¬ä¸€å¤©èµ·ç®—çš„å¤©æ•¸ï¼Œç›´æ¥ä»¥ tanh(txn_date / 60) åšå…¨åŸŸæ¨™æº–åŒ–
    day_pos = [math.tanh(float(0)/60.0) if d == 0.5 else math.tanh(float(d)/60.0) if d != -1 else -1 for d in delta_days]
    #print(f'day_pos = {day_pos}')

    # ----------------------------- äº¤æ˜“æ™‚é–“ (Time2Vec) -----------------------------
    t2v = []
    for i, t in enumerate(df['txn_time']):
        if df.loc[i, 'role'] == 'PAD':
            t2v.append([0.0, 0.0])
            continue
        try:
            h, m, _ = map(int, t.split(":"))
        except:
            h, m = 0, 0
        t2v.append(time2vec_scalar(h, m))

    actual_len = len(df[df['role'] != 'PAD'])
    mask = [1 if r != 'PAD' else 0 for r in df['role']]

    result = {
        "acct": acct,
        "txn_type": txn_type,
        "channel": channel_idx,
        "currency": curr_idx,
        "is_twd": is_twd,
        "amt_norm": amt_norm,
        "same_person": same_person,
        "delta_days_value": delta_days_value,
        "time2vec": t2v,
        "seq_len": actual_len,
        "mask": mask,
        "day_pos": day_pos,
        }
    return result

# ========= MAIN PIPELINE =========

def main(args):

    # å°‡ argparse å‚³å…¥çš„å€¼æ›´æ–°å…¨åŸŸè®Šæ•¸
    seed = args.seed    
    samples = args.sample_size
    seq_len = args.seq_len
    
    # è¨­å®šéš¨æ©Ÿè®Šæ•¸seed
    random.seed(seed)
    np.random.seed(seed)

    # è‡ªå‹•å»ºç«‹è³‡æ–™è³‡æ–™å¤¾ï¼ˆä¾ sample_sizeã€seq_lenã€soft_label å‘½åï¼‰
    if args.predict_data:
        sample_dir = f'predict_data'
    else:
        sample_dir = f'sample_{args.sample_size}'

    if args.soft_label > 0:
        data_dir = Path(f"datasets/initial_competition/{sample_dir}/{sample_dir}_seq_len_{args.seq_len}_soft_label_{args.soft_label}")
    else:
        data_dir = Path(f"datasets/initial_competition/{sample_dir}/{sample_dir}_seq_len_{args.seq_len}")

    json_dir = Path(f"datasets/initial_competition/{sample_dir}/{sample_dir}_seq_len_{args.seq_len}")
    test_dir = f"datasets/initial_competition/Esun_test"
    os.makedirs(json_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)

    if args.one_token_per_day:
        otpd = "_one_token_per_day"
    else:
        otpd = ""


    if args.resplit_data:
        TRAIN_JSON = f"{json_dir}/train{otpd}_resplit.json"
        VAL_JSON = f"{json_dir}/val{otpd}_resplit.json"
        TRAIN_NPZ = data_dir / f'train{otpd}_resplit.npz'
        VAL_NPZ = data_dir / f'val{otpd}_resplit.npz'
    else:
        TRAIN_JSON = f"{json_dir}/train{otpd}.json"
        VAL_JSON = f"{json_dir}/val{otpd}.json"
        TRAIN_NPZ = data_dir / f'train{otpd}.npz'
        VAL_NPZ = data_dir / f'val{otpd}.npz'

    if args.one_token_per_day:
        OTPD="_one_token_per_day"
    else:
        OTPD=""

    TEST_JSON = f"datasets/initial_competition/Esun_test/Esun_test_seq_{seq_len}{OTPD}.json"
    TEST_NPZ = f"datasets/initial_competition/Esun_test/Esun_test_seq_{seq_len}{OTPD}.npz"

    start_time = time.time()
    print("ğŸ” è¼‰å…¥å¸³è™Ÿåˆ†é¡è³‡è¨Š...")

    with open(MAX_MONEY_JSON, "r", encoding="utf-8") as f:
        global_currency_max = json.load(f)

    with open(EXCHANGE_JSON, "r", encoding="utf-8") as f:
        global_exchange = json.load(f)

    all_accts = load_rank_csv(RANK_DIR / "rank_å…¨éƒ¨_äº¤æ˜“ç­†æ•¸_asc.csv")
    yu_accts = load_rank_csv(RANK_DIR / "rank_ç‰å±±å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
    alert_accts = load_rank_csv(RANK_DIR / "rank_è­¦ç¤ºå¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
    predict_accts = load_rank_csv(RANK_DIR / "rank_å¾…é æ¸¬å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")

    print(f"å…¨éƒ¨å¸³è™Ÿ: {len(all_accts)} | ç‰å±±: {len(yu_accts)} | è­¦ç¤º: {len(alert_accts)} | å¾…é æ¸¬: {len(predict_accts)}")

    # Load meta index
    with open(INDEX_JSON, "r") as f:
        meta = json.load(f)
    index_map = meta["index"]
    
    if not os.path.exists(TRAIN_NPZ) or not os.path.exists(VAL_NPZ):
    # ç¯©é¸è¨“ç·´å¸³æˆ¶
        if not os.path.exists(TRAIN_JSON) or not os.path.exists(VAL_JSON):
            candidate_accts = list(yu_accts - alert_accts - predict_accts)
            print(f"å¯ç”¨éè­¦ç¤ºç‰å±±å¸³æˆ¶æ•¸: {len(candidate_accts)}")
            print(f'\næœªæ‰¾åˆ°{TRAIN_JSON}ã€{VAL_JSON}')
            if args.predict_data:
                
                predict_rank_df = pd.read_csv(RANK_DIR / "rank_å¾…é æ¸¬å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
                results = []
                for i, acct in enumerate(tqdm(predict_accts, desc="è½‰æ›å¾…é æ¸¬å¸³æˆ¶ä¸­...")):
                    if acct not in index_map:
                        continue
                    if args.one_token_per_day:
                        res = process_account_per_day(args, acct, meta, index_map[acct], global_exchange)
                    else:
                        res = process_account(args, acct, meta, index_map[acct], global_exchange)
                    txn_cnt = int(predict_rank_df.loc[predict_rank_df["acct"] == acct, "total_txn_count"].values[0])
                    res["bucket"] = bucket_txn_count(txn_cnt)
                    results.append(res)
            else:
                # ç¯©é¸æ¯æ—¥å¹³å‡äº¤æ˜“é‡ < 20
                rank_df = pd.read_csv(RANK_DIR / "rank_ç‰å±±å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
                rank_df["avg_txn_per_day"] = rank_df["total_txn_count"] / rank_df["day_span"]
                filtered = rank_df[rank_df["avg_txn_per_day"] < 20]
                candidate_accts = set(filtered["acct"].tolist()) - alert_accts - predict_accts
                
                # --- å»ºç«‹ bucket ç¾¤çµ„ ---
                bucket_groups = {}
                for _, row in filtered.iterrows():
                    acct = row["acct"]
                    if acct in alert_accts or acct in predict_accts:
                        continue
                    b = bucket_txn_count(row["total_txn_count"])
                    bucket_groups.setdefault(b, []).append(acct)

                # --- åˆ†å±¤æŠ½æ¨£ï¼Œæ¯å€‹ bucket è‡³å°‘å– 50 ç­† ---
                sampled_accts = []
                total_count = sum(len(v) for v in bucket_groups.values())
                for b, accts in bucket_groups.items():
                    p = len(accts) / total_count
                    n = max(50, int(samples * p))
                    sampled_accts.extend(random.sample(accts, min(n, len(accts))))
                print(f"åˆ†å±¤æŠ½æ¨£å®Œæˆï¼Œå…±å– {len(sampled_accts)} ç­†å¸³æˆ¶ (è¦†è“‹ {len(bucket_groups)} å€‹ bucket)")

                # å–æ¨£ 2è¬ç­†
                if len(sampled_accts) > samples:
                    sampled_accts = random.sample(sampled_accts, samples)
                print(f"éš¨æ©ŸæŠ½æ¨£å¸³æˆ¶æ•¸: {len(sampled_accts)}")

                # è™•ç†å¸³æˆ¶è³‡æ–™
                results = []
                for i, acct in enumerate(tqdm(sampled_accts[:], desc="è½‰æ›ä¸­...")):
                    if acct not in index_map:
                        continue
                    if args.one_token_per_day:
                        res = process_account_per_day(args, acct, meta, index_map[acct], global_exchange)
                    else:
                        res = process_account(args, acct, meta, index_map[acct], global_exchange)
                    # è¨˜éŒ„å¸³æˆ¶æ‰€å±¬ bucket
                    txn_cnt = int(rank_df.loc[rank_df["acct"] == acct, "total_txn_count"].values[0])
                    res["bucket"] = bucket_txn_count(txn_cnt)
                    results.append(res)

            # === è™•ç†è­¦ç¤ºå¸³æˆ¶ ===
            print("\nâš ï¸ é–‹å§‹è™•ç†è­¦ç¤ºå¸³æˆ¶...")
            alert_results = []
            alert_rank_df = pd.read_csv(RANK_DIR / "rank_è­¦ç¤ºå¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")

            for i, acct in enumerate(tqdm(alert_accts, desc="è½‰æ›è­¦ç¤ºå¸³æˆ¶ä¸­...")):
                if acct not in index_map:
                    continue
                if args.one_token_per_day:
                    res = process_account_per_day(args, acct, meta, index_map[acct], global_exchange)
                else:
                    res = process_account(args, acct, meta, index_map[acct], global_exchange)
                txn_cnt = int(alert_rank_df.loc[alert_rank_df["acct"] == acct, "total_txn_count"].values[0])
                res["bucket"] = bucket_txn_count(txn_cnt)
                alert_results.append(res)
                if (i+1) % 200 == 0:
                    elapsed = time.time() - start_time
                    est_total = elapsed / (i+1) * len(alert_accts)
                    #print(f"âœ… å·²å®Œæˆ {i+1}/{len(alert_accts)} | é ä¼°å‰©é¤˜: {est_total - elapsed:.1f} ç§’")

            print(f"âœ… è­¦ç¤ºå¸³æˆ¶è™•ç†å®Œæˆï¼Œå…± {len(alert_results)} ç­†")


            # åˆ†å‰² train/val
            if args.resplit_data:
                # --- åˆ†å±¤åˆ‡åˆ†ï¼šä¸€èˆ¬å¸³æˆ¶ ---
                train_data_normal, val_data_normal = [], []
                from collections import defaultdict

                bucket_map_normal = defaultdict(list)
                for r in results:  # ä¸€èˆ¬å¸³æˆ¶
                    bucket_map_normal[r["bucket"]].append(r)

                for b, items in bucket_map_normal.items():
                    random.shuffle(items)
                    split_idx = int(len(items) * 0.9)
                    train_data_normal.extend(items[:split_idx])
                    val_data_normal.extend(items[split_idx:])

                # --- åˆ†å±¤åˆ‡åˆ†ï¼šè­¦ç¤ºå¸³æˆ¶ ---
                train_data_alert, val_data_alert = [], []
                bucket_map_alert = defaultdict(list)
                for r in alert_results:  # è­¦ç¤ºå¸³æˆ¶
                    bucket_map_alert[r["bucket"]].append(r)

                for b, items in bucket_map_alert.items():
                    random.shuffle(items)
                    split_idx = int(len(items) * 0.9)
                    train_data_alert.extend(items[:split_idx])
                    val_data_alert.extend(items[split_idx:])

                # --- åˆä½µ ---
                train_data = train_data_normal + train_data_alert
                val_data = val_data_normal + val_data_alert
            else:
                # --- åˆä½µä¸€èˆ¬å¸³æˆ¶èˆ‡è­¦ç¤ºå¸³æˆ¶ ---
                all_results = results + alert_results

                # åˆ†å‰² train/val
                # --- åˆ†å±¤åˆ‡åˆ† (æ¯å€‹ bucket å„è‡ª 9:1) ---
                train_data, val_data = [], []
                from collections import defaultdict
                bucket_map = defaultdict(list)
                for r in all_results:
                    bucket_map[r["bucket"]].append(r)

                for b, items in bucket_map.items():
                    random.shuffle(items)
                    split_idx = int(len(items) * 0.9)
                    train_data.extend(items[:split_idx])
                    val_data.extend(items[split_idx:])    

            # --- å„²å­˜ ---
            with open(TRAIN_JSON, "w") as f:
                json.dump(train_data, f)
            with open(VAL_JSON, "w") as f:
                json.dump(val_data, f)
                    
            print(f"âœ… å„²å­˜å®Œæˆ: train.json({len(train_data)}) / val.json({len(val_data)})")
            print("è™•ç†æ™‚é–“: %.2f ç§’" % (time.time() - start_time))

        else:
            print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨çš„è¨“ç·´èˆ‡é©—è­‰è³‡æ–™ï¼Œç›´æ¥è¼‰å…¥: {TRAIN_JSON}ã€{VAL_JSON}")
            with open(TRAIN_JSON, "r", encoding="utf-8") as f:
                train_data = json.load(f)
            with open(VAL_JSON, "r", encoding="utf-8") as f:
                val_data = json.load(f)
            print(f"âœ… å·²è¼‰å…¥ {len(train_data)} ç­†è¨“ç·´è³‡æ–™ã€{len(val_data)} ç­†é©—è­‰è³‡æ–™ã€")

        print("ğŸ”„ è½‰æ›æˆ token åºåˆ—ä¸­... (å°šæœª embedding)")

        train_tokens, train_masks, train_labels, train_accts = flatten_tokens(args, train_data, alert_accts, mode="train", soft_label=args.soft_label)
        np.savez(TRAIN_NPZ, tokens=train_tokens, mask=train_masks, label=train_labels, acct=train_accts)
        print(f"âœ… å„²å­˜å®Œæˆ: train.npz ({train_tokens.shape})")

        val_tokens, val_masks, val_labels, val_accts = flatten_tokens(args, val_data, alert_accts, mode="val", soft_label=0)   
        np.savez(VAL_NPZ, tokens=val_tokens, mask=val_masks, label=val_labels, acct=val_accts)
        print(f"âœ… å„²å­˜å®Œæˆ: val.npz ({val_tokens.shape})")

        print("Train_Val è™•ç†æ™‚é–“: %.2f ç§’" % (time.time() - start_time))
    else:
        print(f"train.npz å·²å­˜åœ¨:{TRAIN_NPZ}")
        print(f"val.npz å·²å­˜åœ¨:{VAL_NPZ}")
    
    if not os.path.exists(TEST_NPZ):
        start_time = time.time()
        if not os.path.exists(TEST_JSON):
            # === è™•ç†å¾…é æ¸¬å¸³æˆ¶ (test set) ===
            print("\nğŸ” é–‹å§‹è™•ç†æ¸¬è©¦è³‡æ–™(å¾…é æ¸¬å¸³æˆ¶)...")
            test_results = []
            predict_rank_df = pd.read_csv(RANK_DIR / "rank_å¾…é æ¸¬å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")

            for i, acct in enumerate(tqdm(predict_accts, desc="è½‰æ›å¾…é æ¸¬å¸³æˆ¶ä¸­...")):
                if acct not in index_map:
                    continue
                if args.one_token_per_day:
                    res = process_account_per_day(args, acct, meta, index_map[acct], global_exchange)
                else:
                    res = process_account(args, acct, meta, index_map[acct], global_exchange)
                txn_cnt = int(predict_rank_df.loc[predict_rank_df["acct"] == acct, "total_txn_count"].values[0])
                res["bucket"] = bucket_txn_count(txn_cnt)
                test_results.append(res)
                if (i+1) % 200 == 0:
                    elapsed = time.time() - start_time
                    est_total = elapsed / (i+1) * len(predict_accts)
                    #print(f"âœ… å·²å®Œæˆ {i+1}/{len(predict_accts)} | é ä¼°å‰©é¤˜: {est_total - elapsed:.1f} ç§’")

            print(f"âœ… å¾…é æ¸¬å¸³æˆ¶è™•ç†å®Œæˆï¼Œå…± {len(test_results)} ç­†")

            # å„²å­˜ JSON
            with open(TEST_JSON, "w") as f:
                json.dump(test_results, f)
            print(f"âœ… å„²å­˜å®Œæˆ: Esun_test.json({len(test_results)})")
        else:
            # === è¼‰å…¥å·²å­˜åœ¨çš„ test JSON ===
            print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨çš„æ¸¬è©¦è³‡æ–™ï¼Œç›´æ¥è¼‰å…¥: {TEST_JSON}")
            with open(TEST_JSON, "r", encoding="utf-8") as f:
                test_results = json.load(f)
            print(f"âœ… å·²è¼‰å…¥ {len(test_results)} ç­†å¾…é æ¸¬å¸³æˆ¶è³‡æ–™")

        print("ğŸ”„ è½‰æ›æˆ token åºåˆ—ä¸­... (å°šæœª embedding)")

        test_tokens, test_masks, test_labels, test_accts = flatten_tokens(args, test_results, alert_accts, mode="test", soft_label=0)
        np.savez(TEST_NPZ, tokens=test_tokens, mask=test_masks, label=test_labels, acct=test_accts)
        print(f"âœ… å„²å­˜å®Œæˆ: test.npz ({test_tokens.shape})")

        print("Esun_test è™•ç†æ™‚é–“: %.2f ç§’" % (time.time() - start_time))
    
    else:
        print(f"Test.npz å·²å­˜åœ¨:{TEST_NPZ}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Data preprocessing pipeline for Esun competition")

    # âœ… å¯èª¿æ•´çš„åƒæ•¸
    parser.add_argument("--sample_size", type=int, default=20000, help="æŠ½æ¨£å¸³æˆ¶æ•¸é‡")
    parser.add_argument("--seq_len", type=int, default=100, help="æ¯å¸³æˆ¶åºåˆ—é•·åº¦")
    parser.add_argument("--seed", type=int, default=42, help="random seed")
    parser.add_argument("--one_token_per_day", type=str2bool, default=False, help="æ˜¯å¦å°‡ç‰¹å¾µæ”¹æˆæ¯æ—¥å½™æ•´")
    parser.add_argument("--predict_data", type=str2bool, default=False, help="æ˜¯å¦ä½¿ç”¨å¾…é æ¸¬å¸³æˆ¶ä½œç‚ºè¨“ç·´è³‡æ–™")
    parser.add_argument("--soft_label", type=float, default=0, help="éè­¦ç¤ºå¸³æˆ¶ soft label å€¼ (è‹¥ <=0 å‰‡ç‚º hard label)")
    parser.add_argument("--resplit_data", type=str2bool, default=False, help="æ˜¯å¦å°‡è­¦ç¤ºèˆ‡æ­£å¸¸å¸³æˆ¶å„è‡ªæŒ‰ç…§äº¤æ˜“ç­†æ•¸åˆ†ç¾¤?")

    args = parser.parse_args()
    
    # åŸ·è¡Œä¸»æµç¨‹
    main(args)