#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dataloader.py
è³‡æ–™å‰è™•ç†èˆ‡ JSON è¼¸å‡º
"""

import os
import json
import time
import math
import random
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import argparse

# ========= CONFIG =========
CACHE_DIR = Path("Preprocess/cache")
DETAILS_DIR = CACHE_DIR / "details"
RANK_DIR = CACHE_DIR / "ranks"
INDEX_JSON = CACHE_DIR / "account_index.json"
DATAFILES_DIR = Path("datafiles")
EXCHANGE_JSON = DATAFILES_DIR / "exchange_rate.json"

GLOBAL_CHANNELS = ["PAD", "01", "02", "03", "04", "05", "06", "07", "99", "UNK"]
CHANNEL_CODE = [-1, 1, 2, 3, 4, 5, 6, 7, 8, 0]
CHANNEL_MAP = {c: i for i, c in zip(CHANNEL_CODE, GLOBAL_CHANNELS)}

def str2bool(v):
    """
    å°‡å­—ä¸²è½‰æ›ç‚ºå¸ƒæ—å€¼ã€‚

    æ”¯æ´çš„å­—ä¸²åŒ…å«ï¼š
        True é¡å‹ï¼š'yes', 'true', 't', 'y', '1'
        False é¡å‹ï¼š'no', 'false', 'f', 'n', '0'
    è‹¥è¼¸å…¥å¸ƒæ—å€¼å‰‡ç›´æ¥å›å‚³ã€‚
    è‹¥ç„¡æ³•è§£æå‰‡æ‹‹å‡º argparse.ArgumentTypeErrorã€‚

    åƒæ•¸:
        v (str | bool): è¦è½‰æ›çš„å€¼ã€‚

    å›å‚³:
        bool: è§£æå¾Œçš„å¸ƒæ—å€¼ã€‚
    """
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

# ========= UTILS =========
def load_rank_csv(path):    
    """
    è¼‰å…¥å¸³è™Ÿæ’å CSVï¼Œä¸¦å›å‚³ acct æ¬„ä½çš„é›†åˆã€‚

    åƒæ•¸:
        path (str | Path): CSV æª”æ¡ˆè·¯å¾‘ã€‚

    å›å‚³:
        set[str]: å¸³è™Ÿå­—ä¸²é›†åˆã€‚
    """
    df = pd.read_csv(path)
    return set(df['acct'].astype(str).tolist())

def piecewise_norm(val_twd):
    """
    ä¾ç…§é‡‘é¡ (å°å¹£) é€²è¡Œåˆ†æ®µç·šæ€§ç¸®æ”¾ã€‚

    åˆ†æ®µè¦å‰‡ï¼š
        100 â†’ 0.05
        1000 â†’ 0.25
        ...
        1å„„ä»¥ä¸Š â†’ 1.0

    åƒæ•¸:
        val_twd (float): é‡‘é¡ (å°å¹£)ã€‚

    å›å‚³:
        float: åˆ†æ®µç¸®æ”¾çµæœï¼Œç¯„åœ 0~1ã€‚
    """
    thresholds = [100, 1_000, 10_000, 100_000, 1_000_000, 10_000_000, 100_000_000]
    scales =     [0.05, 0.25, 0.45, 0.65, 0.85, 0.95, 1.0]
    if val_twd <= thresholds[0]:
        return (val_twd / thresholds[0]) * scales[0]
    for i in range(1, len(thresholds)):
        if val_twd < thresholds[i]:
            r = (val_twd - thresholds[i-1]) / (thresholds[i] - thresholds[i-1])  
            return scales[i-1] + r * (scales[i] - scales[i-1])
    return 1.0

# --- å®šç¾©äº¤æ˜“ç­†æ•¸ bucket ---
def bucket_txn_count(n):
    """
    ä¾æ“šäº¤æ˜“ç­†æ•¸å°‡å¸³æˆ¶åˆ†å…¥å°æ‡‰ bucketã€‚

    åˆ†é¡ç¯„ä¾‹:
        1 â†’ 'b1'
        2 â†’ 'b2'
        3~5 â†’ 'b3_5'
        ...
        >=500 â†’ 'b500p'

    åƒæ•¸:
        n (int): äº¤æ˜“ç­†æ•¸ã€‚

    å›å‚³:
        str: bucket æ¨™ç±¤ã€‚
    """
    if n == 1: return "b1"
    elif n == 2: return "b2"
    elif 3 <= n <= 5: return "b3_5"
    elif 6 <= n <= 10: return "b6_10"
    elif 11 <= n <= 20: return "b11_20"
    elif 21 <= n <= 50: return "b21_50"
    elif 51 <= n <= 100: return "b51_100"
    elif 101 <= n <= 500: return "b101_500"
    else: return "b500p"

def normalize_money(x, curr_list, exchange_rate_json, default_currency="TWD", mode="piecewise"):
    """
    å°‡é‡‘é¡ä¾å¹£åˆ¥è½‰æ›ç‚ºå°å¹£å¾Œï¼Œå¥—ç”¨åˆ†æ®µç¸®æ”¾å‡½å¼é€²è¡Œæ­£è¦åŒ–ã€‚

    æµç¨‹:
        1. ä¾å¹£åˆ¥æŸ¥åŒ¯ç‡æ›ç®—æˆå°å¹£ã€‚
        2. å¥—ç”¨ piecewise_norm() æ˜ å°„è‡³ 0 ~ 1ã€‚

    åƒæ•¸:
        x (list[float]): é‡‘é¡åˆ—è¡¨ã€‚
        curr_list (list[str]): å¹£åˆ¥åˆ—è¡¨ã€‚
        exchange_rate_json (dict): å¹£åˆ¥å° TWD åŒ¯ç‡ã€‚
        default_currency (str): é è¨­å°å¹£ä»£ç¢¼ã€‚
        mode (str): å¯æ“´å……ï¼Œé è¨­ 'piecewise'ã€‚

    å›å‚³:
        list[float]: æ­£è¦åŒ–å¾Œé‡‘é¡ã€‚
    """
    result = []
    for val, cur in zip(x, curr_list):
        rate = exchange_rate_json.get(cur, exchange_rate_json.get(default_currency, 1.0))
        val_twd = val * rate
        norm = piecewise_norm(val_twd)
        result.append(norm)
    return result

def time2vec_scalar(hour, minute):
    """
    åŸºç¤ç‰ˆ Time2Vecï¼šå°‡æ™‚é–“ (æ™‚ã€åˆ†) æ˜ å°„ç‚º sin/cos å…©ç¶­å‘é‡ã€‚

    åƒæ•¸:
        hour (int): å°æ™‚ (0~23)ã€‚
        minute (int): åˆ†é˜ (0~59)ã€‚

    å›å‚³:
        list[float]: [sin(value), cos(value)] æ™‚é–“è½‰æ›å¾Œçš„å‘é‡
    """
    val = hour * 60 + minute
    return [math.sin(val / 1440 * math.pi), math.cos(val / 1440 * math.pi)]

# ========= DATA PREPROCESS =========

def process_account(args, acct, meta, index_info, global_exchange):
    """
    å°‡å–®ä¸€å¸³æˆ¶çš„äº¤æ˜“ç´€éŒ„è½‰æ›æˆæ¨¡å‹å¯ä½¿ç”¨çš„åºåˆ—ç‰¹å¾µæ ¼å¼ã€‚

    åŠŸèƒ½:
        - è®€å–å°æ‡‰äº¤æ˜“æ˜ç´° CSV
        - å–æœ€å¾Œ seq_len ç­†äº¤æ˜“ï¼Œä¸¦é€²è¡Œ padding
        - ç”¢ç”Ÿå„é …ç‰¹å¾µ (äº¤æ˜“å‹åˆ¥ã€é€šè·¯ indexã€é‡‘é¡æ­£è¦åŒ–ã€Time2Vecã€å¤©æ•¸å·®ç­‰)
        - å»ºç«‹ maskã€åºåˆ—é•·åº¦ç­‰è³‡è¨Š

    åƒæ•¸:
        args: argparse è¨­å®šåƒæ•¸ã€‚
        acct (str): å¸³è™Ÿ IDã€‚
        meta (dict): meta JSON å…¨é«”è³‡è¨Šã€‚
        index_info (dict): acct å°æ‡‰çš„æª”æ¡ˆèˆ‡èµ·è¨– indexã€‚
        global_exchange (dict): å¹£åˆ¥åŒ¯ç‡è¡¨ã€‚

    å›å‚³:
        dict: åŒ…å«æ‰€æœ‰æ¨¡å‹ç‰¹å¾µçš„å­—å…¸ã€‚
    """

    file_path = os.path.join(DETAILS_DIR, index_info['file'])
    start, end = index_info['start'], index_info['end']
    df = pd.read_csv(file_path).iloc[start:end].reset_index(drop=True)
    # åƒ…å–æœ€å¾Œ 50 ç­†
    df = df.tail(args.seq_len).reset_index(drop=True)

    # å¡«è£œ padding
    pad_len = args.seq_len - len(df)
    if pad_len > 0:
        pad = pd.DataFrame([{
            'txn_amt': 0,
            'currency_type': 'PAD',
            'is_self_txn': 'UNK',
            'channel_type': 'UNK',
            'txn_date': -1,
            'txn_time': '00:00:00',
            'role': 'PAD'
        }] * pad_len)
        df = pd.concat([pad, df], ignore_index=True)

    # ===== Feature Transform =====
    # äº¤æ˜“å‹åˆ¥
    txn_type = df['role'].apply(lambda x: 1 if x == 'OUT' else (0 if x == 'IN' else -1)).tolist()
    # é€šè·¯ embedding index
    channel_idx = df['channel_type'].apply(lambda x: CHANNEL_MAP.get(x, 0)).tolist()
    # å¹£åˆ¥ embedding index
    curr_map = {c: i for i, c in enumerate(sorted(df['currency_type'].unique()))}
    curr_idx = df['currency_type'].apply(lambda x: curr_map.get(x, 0)).tolist()
    # æ˜¯å¦å°å¹£
    is_twd = df['currency_type'].apply(lambda x: 1 if x == 'TWD' else (0 if x != 'PAD' else -1)).tolist()
    # é‡‘é¡
    amt_norm = normalize_money(df['txn_amt'].tolist(),
                                df['currency_type'].tolist(),
                                global_exchange
                                )
    # æ˜¯å¦åŒäºº
    same_person = df['is_self_txn'].apply(lambda x: 1 if x == 'Y' else (0 if x == 'N' else -1)).tolist()
    # ----------------------------- å·®è·å¤©æ•¸ bucket ---------------------------------
    # å…ˆç¢ºä¿ txn_date å·²æ’åº
    #df = df.sort_values('txn_date').reset_index(drop=True)

    days = df['txn_date'].astype(float).tolist()
    #print(f"days = {days}")
    delta_days = []
    for i in range(len(df)):
        if df.loc[i, 'role'] == 'PAD':          # padding token
            delta_days.append(-1)
        elif days[i-1] == -1:
            delta_days.append(0.5)                # ç¬¬ä¸€ç­†
        else:
            d = days[i] - days[i-1]
            #print(f'days[i] = {days[i]}')
            #print(f'days[i-1] = {days[i-1]}')
            if d == 0:
                delta_days.append(0)          # åŒä¸€å¤©äº¤æ˜“
            else:
                delta_days.append(d)
                    
    # ----------------------------- å·®è·å¤©æ•¸ç­‰æ¯”ä¾‹æ˜ å°„ [-1, 1] -----------------------------
    delta_days_value = []
    for diff in delta_days:
        if diff == -1:
            delta_days_value.append(-1.0)
            continue
        if diff == 0.5:
            delta_days_value.append(0.0)   # é¦–ç­†
            continue

        if diff == 0:
            val = 0.1                      # åŒæ—¥
        elif diff == 1:
            val = 0.2
        elif 2 <= diff <= 3:
            val = 0.3
        elif 4 <= diff <= 7:
            val = 0.4
        elif 8 <= diff <= 10:
            val = 0.5
        elif 11 <= diff <= 20:
            val = 0.6
        elif 21 <= diff <= 40:
            val = 0.7
        elif 41 <= diff <= 70:
            val = 0.8
        elif 71 <= diff <= 100:
            val = 0.9
        elif diff >= 101:
            val = 1.0
        else:
            val = 0.0
        delta_days_value.append(val)
    # ----------------------------- å±€éƒ¨ day_position -----------------------------
    # txn_date ç‚ºåˆ‡é½Šç¬¬ä¸€å¤©èµ·ç®—çš„å¤©æ•¸ï¼Œç›´æ¥ä»¥ tanh(txn_date / 60) åšå…¨åŸŸæ¨™æº–åŒ–
    day_pos = [math.tanh(float(0)/60.0) if d == 0.5 else math.tanh(float(d)/60.0) if d != -1 else -1 for d in delta_days]
    #print(f'day_pos = {day_pos}')

    # ----------------------------- äº¤æ˜“æ™‚é–“ (Time2Vec) -----------------------------
    t2v = []
    for i, t in enumerate(df['txn_time']):
        if df.loc[i, 'role'] == 'PAD':
            t2v.append([0.0, 0.0])
            continue
        try:
            h, m, _ = map(int, t.split(":"))
        except:
            h, m = 0, 0
        t2v.append(time2vec_scalar(h, m))

    actual_len = len(df[df['role'] != 'PAD'])
    mask = [1 if r != 'PAD' else 0 for r in df['role']]

    result = {
        "acct": acct,
        "txn_type": txn_type,
        "channel": channel_idx,
        "currency": curr_idx,
        "is_twd": is_twd,
        "amt_norm": amt_norm,
        "same_person": same_person,
        "delta_days_value": delta_days_value,
        "time2vec": t2v,
        "seq_len": actual_len,
        "mask": mask,
        "day_pos": day_pos,
        }
    return result


def flatten_tokens(args, dataset, alert_accts, mode="train", soft_label=0.3):
    """
    å°‡å¸³æˆ¶ç´šåˆ¥è³‡æ–™å±•é–‹æˆå›ºå®š (N, seq_len, 10) çš„ token å¼µé‡ã€‚

    åŠŸèƒ½:
        - å°‡ feature dict è½‰æ›ç‚ºæ¨¡å‹å¯ç”¨çš„ token tensor
        - ç”¢ç”Ÿ maskã€labelï¼ˆè­¦ç¤ºå¸³æˆ¶ = 1ï¼‰
        - å›å‚³ tokens / masks / labels / acct list

    åƒæ•¸:
        args: argparse è¨­å®šã€‚
        dataset (list[dict]): ç¶“ process_account è™•ç†å¾Œçš„è³‡æ–™åˆ—è¡¨ã€‚
        alert_accts (set[str]): è­¦ç¤ºå¸³æˆ¶é›†åˆã€‚
        mode (str): train/val/testã€‚
        soft_label (float): å¯ç”¨æ–¼ soft labelingï¼ˆç›®å‰æœªä½¿ç”¨ï¼‰ã€‚

    å›å‚³:
        tuple:
            tokens (np.ndarray)
            masks (np.ndarray)
            labels (np.ndarray)
            accts (np.ndarray)
        ç”¨æ–¼æ¨¡å‹è¨“ç·´/æ¨è«–çš„tokenæ ¼å¼
    """

    # å°‡å¸³æˆ¶ç´šåˆ¥è³‡æ–™è½‰ç‚º (N, 50, 10) tokens
    tokens, masks, labels, accts = [], [], [], []
    for r in dataset:
        N = len(r["txn_type"])  # SEQ_LEN
        tok = []
        for i in range(N):
            sin_val, cos_val = r["time2vec"][i]
            tok.append([
                sin_val, cos_val,                   # 2 ç¶­ äº¤æ˜“æ™‚é–“
                r["day_pos"][i],                    # 1    äº¤æ˜“å¤©æ•¸ (èˆ‡ç•¶å‰æ‰€æœ‰äº¤æ˜“ç›¸æ¯”)
                r["txn_type"][i],                   # 1    äº¤æ˜“å‹åˆ¥ (æ”¶/åŒ¯æ¬¾)
                r["channel"][i],                    # 1    äº¤æ˜“é€šè·¯
                r["currency"][i],                   # 1    äº¤æ˜“å¹£åˆ¥
                r["is_twd"][i],                     # 1    æ˜¯å¦ç‚ºå°å¹£
                r["amt_norm"][i],                   # 1    é‡‘é¡
                r["delta_days_value"][i],           # 1    èˆ‡ä¸Šç­†äº¤æ˜“å·®ç•°å¤©æ•¸
                r["same_person"][i],                # 1    æ˜¯å¦ç‚ºåŒä¸€äºº
            ])
        tokens.append(tok)
        masks.append(r["mask"])
        label = 1 if r["acct"] in alert_accts else 0 # æ¨™ç±¤ï¼šè­¦ç¤ºå¸³æˆ¶ç‚º1ï¼Œå…¶é¤˜0
        labels.append(label)
        accts.append(r["acct"])

    return (
        np.array(tokens, dtype=np.float32),
        np.array(masks, dtype=np.int8),
        np.array(labels, dtype=np.float32),
        np.array(accts)
    )
# ========= MAIN PIPELINE =========

def main(args):
    """
    åŸ·è¡Œè³‡æ–™å‰è™•ç†å®Œæ•´æµç¨‹ã€‚

    åŠŸèƒ½:
        - è¼‰å…¥è³‡æ–™ã€åŒ¯ç‡ã€å¸³è™Ÿåˆ†é¡ CSV
        - ä¾ bucket èˆ‡è­¦ç¤ºå¸³è™Ÿé€²è¡Œåˆ†å±¤æŠ½æ¨£
        - ç”¢ç”Ÿ train / val / test çš„ JSON èˆ‡ NPZ
        - å‘¼å« process_account() èˆ‡ flatten_tokens()
        - å°è³‡æ–™é€²è¡Œåºåˆ—ç‰¹å¾µè½‰æ›

    åƒæ•¸:
        args: argparse è§£æçµæœã€‚

    å›å‚³:
        None
    """
    
    # å°‡ argparse å‚³å…¥çš„å€¼æ›´æ–°å…¨åŸŸè®Šæ•¸
    seed = args.seed    
    samples = args.sample_size
    seq_len = args.seq_len
    
    # è¨­å®šéš¨æ©Ÿè®Šæ•¸seed
    random.seed(seed)
    np.random.seed(seed)

    data_dir = args.data_dir
    test_dir = args.test_dir

    os.makedirs(test_dir, exist_ok=True)
    os.makedirs(data_dir, exist_ok=True)

    TRAIN_JSON = f"{data_dir}/train.json"
    TRAIN_NPZ = f"{data_dir}/train.npz"

    VAL_JSON = f"{data_dir}/val.json"
    VAL_NPZ = f"{data_dir}/val.npz"

    TEST_JSON = f"datasets/initial_competition/Esun_test/Esun_test_seq_{seq_len}.json"
    TEST_NPZ = f"datasets/initial_competition/Esun_test/Esun_test_seq_{seq_len}.npz"

    start_time = time.time()
    print("ğŸ” è¼‰å…¥å¸³è™Ÿåˆ†é¡è³‡è¨Š...")

    with open(EXCHANGE_JSON, "r", encoding="utf-8") as f:
        global_exchange = json.load(f)

    all_accts = load_rank_csv(RANK_DIR / "rank_å…¨éƒ¨_äº¤æ˜“ç­†æ•¸_asc.csv")
    Esun_accts = load_rank_csv(RANK_DIR / "rank_ç‰å±±å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
    alert_accts = load_rank_csv(RANK_DIR / "rank_è­¦ç¤ºå¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
    predict_accts = load_rank_csv(RANK_DIR / "rank_å¾…é æ¸¬å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")

    print(f"å…¨éƒ¨å¸³è™Ÿ: {len(all_accts)} | ç‰å±±: {len(Esun_accts)} | è­¦ç¤º: {len(alert_accts)} | å¾…é æ¸¬: {len(predict_accts)}")

    # Load meta index
    with open(INDEX_JSON, "r") as f:
        meta = json.load(f)
    index_map = meta["index"]
    
    if not os.path.exists(TRAIN_NPZ) or not os.path.exists(VAL_NPZ):
    # ç¯©é¸è¨“ç·´å¸³æˆ¶
        if not os.path.exists(TRAIN_JSON) or not os.path.exists(VAL_JSON):
            candidate_accts = list(Esun_accts - alert_accts - predict_accts)
            print(f"å¯ç”¨éè­¦ç¤ºç‰å±±å¸³æˆ¶æ•¸: {len(candidate_accts)}")
            print(f'\næœªæ‰¾åˆ°{TRAIN_JSON}\næœªæ‰¾åˆ°{VAL_JSON}')
            if args.predict_data:
                predict_rank_df = pd.read_csv(RANK_DIR / "rank_å¾…é æ¸¬å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
                results = []
                for i, acct in enumerate(tqdm(predict_accts, desc="è½‰æ›å¾…é æ¸¬å¸³æˆ¶ä¸­...")):
                    if acct not in index_map:
                        continue
                    res = process_account(args, acct, meta, index_map[acct], global_exchange)
                    txn_cnt = int(predict_rank_df.loc[predict_rank_df["acct"] == acct, "total_txn_count"].values[0])
                    res["bucket"] = bucket_txn_count(txn_cnt)
                    results.append(res)
            else:
                Esun_df = pd.read_csv(RANK_DIR / "rank_ç‰å±±å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")
                
                # --- å»ºç«‹ bucket ç¾¤çµ„ ---
                bucket_groups = {}
                for _, row in Esun_df.iterrows():
                    acct = row["acct"]
                    if acct in alert_accts or acct in predict_accts:
                        continue
                    b = bucket_txn_count(row["total_txn_count"])
                    bucket_groups.setdefault(b, []).append(acct)

                # --- åˆ†å±¤æŠ½æ¨£ï¼Œæ¯å€‹ bucket è‡³å°‘å– 50 ç­†ï¼Œæœ€å¤šå– (è©² bucket ä½”å…¨ buckets æ¯”ä¾‹) * samples æ•¸---
                sampled_accts = []
                total_count = sum(len(v) for v in bucket_groups.values())
                for b, accts in bucket_groups.items():
                    p = len(accts) / total_count
                    n = max(50, int(samples * p))
                    sampled_accts.extend(random.sample(accts, min(n, len(accts))))
                print(f"åˆ†å±¤æŠ½æ¨£å®Œæˆï¼Œå…±å– {len(sampled_accts)} ç­†å¸³æˆ¶ (è¦†è“‹ {len(bucket_groups)} å€‹ bucket)")

                # å–æ¨£ 2è¬ç­†
                if len(sampled_accts) > samples:
                    sampled_accts = random.sample(sampled_accts, samples)
                print(f"éš¨æ©ŸæŠ½æ¨£å¸³æˆ¶æ•¸: {len(sampled_accts)}")

                # è™•ç†å¸³æˆ¶è³‡æ–™
                results = []
                for i, acct in enumerate(tqdm(sampled_accts[:], desc="è½‰æ›ä¸­...")):
                    if acct not in index_map:
                        continue
                    res = process_account(args, acct, meta, index_map[acct], global_exchange)
                    # è¨˜éŒ„å¸³æˆ¶æ‰€å±¬ bucket
                    txn_cnt = int(Esun_df.loc[Esun_df["acct"] == acct, "total_txn_count"].values[0])
                    res["bucket"] = bucket_txn_count(txn_cnt)
                    results.append(res)

            # === è™•ç†è­¦ç¤ºå¸³æˆ¶ ===
            print("\nâš ï¸ é–‹å§‹è™•ç†è­¦ç¤ºå¸³æˆ¶...")
            alert_results = []
            alert_rank_df = pd.read_csv(RANK_DIR / "rank_è­¦ç¤ºå¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")

            for i, acct in enumerate(tqdm(alert_accts, desc="è½‰æ›è­¦ç¤ºå¸³æˆ¶ä¸­...")):
                if acct not in index_map:
                    continue
                res = process_account(args, acct, meta, index_map[acct], global_exchange)
                txn_cnt = int(alert_rank_df.loc[alert_rank_df["acct"] == acct, "total_txn_count"].values[0])
                res["bucket"] = bucket_txn_count(txn_cnt)
                alert_results.append(res)

            print(f"âœ… è­¦ç¤ºå¸³æˆ¶è™•ç†å®Œæˆï¼Œå…± {len(alert_results)} ç­†")

            # åˆ†å‰² train/val
            # --- åˆ†å±¤åˆ‡åˆ†ï¼šä¸€èˆ¬å¸³æˆ¶ ---
            train_data_normal, val_data_normal = [], []
            from collections import defaultdict

            bucket_map_normal = defaultdict(list)
            for r in results:  # ä¸€èˆ¬å¸³æˆ¶
                bucket_map_normal[r["bucket"]].append(r)
            for b, items in bucket_map_normal.items():
                random.shuffle(items)
                split_idx = int(len(items) * args.train_ratio)
                train_data_normal.extend(items[:split_idx])
                val_data_normal.extend(items[split_idx:])

            # --- åˆ†å±¤åˆ‡åˆ†ï¼šè­¦ç¤ºå¸³æˆ¶ ---
            train_data_alert, val_data_alert = [], []
            bucket_map_alert = defaultdict(list)
            for r in alert_results:  # è­¦ç¤ºå¸³æˆ¶
                bucket_map_alert[r["bucket"]].append(r)

            for b, items in bucket_map_alert.items():
                random.shuffle(items)
                split_idx = int(len(items) * args.train_ratio)
                train_data_alert.extend(items[:split_idx])
                val_data_alert.extend(items[split_idx:])
            # --- åˆä½µ ---
            train_data = train_data_normal + train_data_alert
            val_data = val_data_normal + val_data_alert

            # --- å„²å­˜ ---
            with open(TRAIN_JSON, "w") as f:
                json.dump(train_data, f)
            with open(VAL_JSON, "w") as f:
                json.dump(val_data, f)
                    
            print(f"âœ… å„²å­˜å®Œæˆ: train.json({len(train_data)}) / val.json({len(val_data)})")
            print("è™•ç†æ™‚é–“: %.2f ç§’" % (time.time() - start_time))

        else:
            print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨çš„è¨“ç·´èˆ‡é©—è­‰è³‡æ–™ï¼Œç›´æ¥è¼‰å…¥: {TRAIN_JSON}ã€{VAL_JSON}")
            with open(TRAIN_JSON, "r", encoding="utf-8") as f:
                train_data = json.load(f)
            with open(VAL_JSON, "r", encoding="utf-8") as f:
                val_data = json.load(f)
            print(f"âœ… å·²è¼‰å…¥ {len(train_data)} ç­†è¨“ç·´è³‡æ–™ã€{len(val_data)} ç­†é©—è­‰è³‡æ–™ã€")

        print("ğŸ”„ è½‰æ›æˆ token åºåˆ—ä¸­... (å°šæœª embedding)")

        train_tokens, train_masks, train_labels, train_accts = flatten_tokens(args, train_data, alert_accts, mode="train")
        np.savez(TRAIN_NPZ, tokens=train_tokens, mask=train_masks, label=train_labels, acct=train_accts)
        print(f"âœ… å„²å­˜å®Œæˆ: train.npz ({train_tokens.shape})")

        val_tokens, val_masks, val_labels, val_accts = flatten_tokens(args, val_data, alert_accts, mode="val", soft_label=0)   
        np.savez(VAL_NPZ, tokens=val_tokens, mask=val_masks, label=val_labels, acct=val_accts)
        print(f"âœ… å„²å­˜å®Œæˆ: val.npz ({val_tokens.shape})")

        print("Train_Val è™•ç†æ™‚é–“: %.2f ç§’" % (time.time() - start_time))
    else:
        print(f"train.npz å·²å­˜åœ¨:{TRAIN_NPZ}")
        print(f"val.npz å·²å­˜åœ¨:{VAL_NPZ}")
    
    if not os.path.exists(TEST_NPZ):
        start_time = time.time()
        if not os.path.exists(TEST_JSON):
            # === è™•ç†å¾…é æ¸¬å¸³æˆ¶ (test set) ===
            print("\nğŸ” é–‹å§‹è™•ç†æ¸¬è©¦è³‡æ–™(å¾…é æ¸¬å¸³æˆ¶)...")
            test_results = []
            predict_rank_df = pd.read_csv(RANK_DIR / "rank_å¾…é æ¸¬å¸³æˆ¶_äº¤æ˜“ç­†æ•¸_asc.csv")

            for i, acct in enumerate(tqdm(predict_accts, desc="è½‰æ›å¾…é æ¸¬å¸³æˆ¶ä¸­...")):
                if acct not in index_map:
                    continue
                res = process_account(args, acct, meta, index_map[acct], global_exchange)
                txn_cnt = int(predict_rank_df.loc[predict_rank_df["acct"] == acct, "total_txn_count"].values[0])
                res["bucket"] = bucket_txn_count(txn_cnt)
                test_results.append(res)
                if (i+1) % 200 == 0:
                    elapsed = time.time() - start_time

            print(f"âœ… å¾…é æ¸¬å¸³æˆ¶è™•ç†å®Œæˆï¼Œå…± {len(test_results)} ç­†")

            # å„²å­˜ JSON
            with open(TEST_JSON, "w") as f:
                json.dump(test_results, f)
            print(f"âœ… å„²å­˜å®Œæˆ: Esun_test.json({len(test_results)})")
        else:
            # === è¼‰å…¥å·²å­˜åœ¨çš„ test JSON ===
            print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨çš„æ¸¬è©¦è³‡æ–™ï¼Œç›´æ¥è¼‰å…¥: {TEST_JSON}")
            with open(TEST_JSON, "r", encoding="utf-8") as f:
                test_results = json.load(f)
            print(f"âœ… å·²è¼‰å…¥ {len(test_results)} ç­†å¾…é æ¸¬å¸³æˆ¶è³‡æ–™")

        print("ğŸ”„ è½‰æ›æˆ token åºåˆ—ä¸­... (å°šæœª embedding)")

        test_tokens, test_masks, test_labels, test_accts = flatten_tokens(args, test_results, alert_accts, mode="test", soft_label=0)
        np.savez(TEST_NPZ, tokens=test_tokens, mask=test_masks, label=test_labels, acct=test_accts)
        print(f"âœ… å„²å­˜å®Œæˆ: test.npz ({test_tokens.shape})")

        print("Esun_test è™•ç†æ™‚é–“: %.2f ç§’" % (time.time() - start_time))
    
    else:
        print(f"Test.npz å·²å­˜åœ¨:{TEST_NPZ}")

if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Data preprocessing pipeline for Esun competition")
    p.add_argument("--data_dir", default="datasets/initial_competition/predict_data/predict_data_seq_len_200/train_ratio_0.9")
    p.add_argument("--test_dir", default="datasets/initial_competition/Esun_test")
    p.add_argument("--predict_data", type=str2bool, default=True, help="æ˜¯å¦ä½¿ç”¨å¾…é æ¸¬å¸³æˆ¶ä½œç‚ºè¨“ç·´è³‡æ–™")
    p.add_argument("--sample_size", type=int, default=0, help="æŠ½æ¨£å¸³æˆ¶æ•¸é‡")
    p.add_argument("--seq_len", type=int, default=200, help="æ¯å¸³æˆ¶åºåˆ—é•·åº¦")
    p.add_argument("--seed", type=int, default=42, help="random seed")
    p.add_argument("--train_ratio", type=float, default=0.9, help="train test split ratio")
    args = p.parse_args()
    
    # åŸ·è¡Œä¸»æµç¨‹
    main(args)