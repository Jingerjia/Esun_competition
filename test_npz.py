import numpy as np

npz_path = "datasets/initial_competition/predict_data/predict_data_seq_len_200/train_resplit.npz"  # â† æ”¹æˆä½ çš„ npz æª”æ¡ˆè·¯å¾‘
data = np.load(npz_path, allow_pickle=True)

print("="*60)
print(f"ğŸ“¦ æª”æ¡ˆ: {npz_path}")
print(f"åŒ…å«çš„ keys: {list(data.files)}")
print("="*60)

# ä¾åºåˆ—å°æ¯å€‹ key çš„åŸºæœ¬è³‡è¨Š
for k in data.files:
    arr = data[k]
    print(f"\nğŸ”¹ Key: '{k}'")
    print(f"  é¡å‹: {type(arr)}")
    if isinstance(arr, np.ndarray):
        print(f"  shape: {arr.shape}, dtype: {arr.dtype}")
        # å¦‚æœæ˜¯ä¸€ç¶­æˆ–äºŒç¶­è³‡æ–™ï¼Œé¡¯ç¤ºå‰å¹¾ç­†å…§å®¹
        if arr.ndim <= 2:
            print(f"  å‰3ç­†è³‡æ–™:\n{arr[:3]}")
        else:
            print(f"  å‰1ç­†è³‡æ–™ shape: {arr[0].shape}")
    else:
        print(f"  (ä¸æ˜¯ ndarray, å…§å®¹ç¤ºä¾‹:) {arr}")

print("\n" + "="*60)
print("âœ… è‹¥æœ‰ key åç‚º 'x' æˆ– 'tokens'ï¼Œè«‹é¡å¤–æª¢æŸ¥å…¶é•·åº¦åˆ†å¸ƒï¼š")

# å˜—è©¦è‡ªå‹•åµæ¸¬å¯èƒ½æ˜¯ token ä¸»é«”çš„ key
for key in ['x', 'tokens', 'input_ids']:
    if key in data.files:
        x = data[key]
        if x.ndim == 2:
            print(f"\nğŸ”¹ {key} shape: {x.shape}")
            token_lens = (x != 0).sum(axis=1)
            print(f"  æœ€çŸ­åºåˆ—é•·åº¦: {token_lens.min()}")
            print(f"  æœ€é•·åºåˆ—é•·åº¦: {token_lens.max()}")
            print(f"  å¹³å‡åºåˆ—é•·åº¦: {token_lens.mean():.2f}")
        elif x.ndim == 3:
            print(f"\nğŸ”¹ {key} shape: {x.shape}")
            token_lens = (x.sum(axis=-1) != 0).sum(axis=1)
            print(f"  æœ€çŸ­åºåˆ—é•·åº¦: {token_lens.min()}")
            print(f"  æœ€é•·åºåˆ—é•·åº¦: {token_lens.max()}")
            print(f"  å¹³å‡åºåˆ—é•·åº¦: {token_lens.mean():.2f}")
