"""
clustering.py
ä½¿ç”¨ dataloader è®€å– npz è³‡æ–™ï¼Œå°ç‰¹å¾µåš clusteringï¼Œ
ä¸¦å°‡èˆ‡ label=1 åŒç¾¤çš„æ¨£æœ¬ soft_label è¨­ç‚º 0.5
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch, SpectralClustering
from kmodes.kmodes import KModes
from kmodes.kprototypes import KPrototypes
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns
from dataloader import get_dataloader

def extract_features_from_dataloader(dataloader):
    """
    å°‡ dataloader è¼‰å…¥çš„ batch ç‰¹å¾µè½‰æ›æˆ 2D å‘é‡ä¾› clustering ä½¿ç”¨
    é€™è£¡ä½¿ç”¨å¹³å‡æ± åŒ– (mean pooling)ï¼Œå°‡åºåˆ—å£“ç¸®æˆå›ºå®šé•·åº¦å‘é‡
    """
    all_features = []
    all_labels = []

    for batch in tqdm(dataloader, desc="ğŸ“¦ Extracting features"):
        x = batch["x"]  # (B, seq_len, feature_dim)
        y = batch["label"]  # (B,)
        features = x.mean(dim=1)  # å¹³å‡æ™‚é–“åºåˆ—
        all_features.append(features.cpu().numpy())
        all_labels.append(y.cpu().numpy())

    X = np.concatenate(all_features, axis=0)
    labels = np.concatenate(all_labels, axis=0)
    print(f"âœ… ç‰¹å¾µæå–å®Œæˆ: {X.shape}")
    return X, labels

def plot_cluster_scatter(X, cluster_ids, labels, save_path="cluster_scatter.png", method="pca"):
    """
    ä½¿ç”¨ PCA æˆ– t-SNE å°‡ç‰¹å¾µé™ç¶­æˆ 2D ä¸¦ç•«å‡º cluster æ•£é»åœ–
    """
    print(f"ğŸ” ä½¿ç”¨ {method.upper()} é™ç¶­ä¸­...")

    if method.lower() == "pca":
        reducer = PCA(n_components=2, random_state=42)
    else:
        reducer = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)

    reduced = reducer.fit_transform(X)

    df = pd.DataFrame({
        "x": reduced[:, 0],
        "y": reduced[:, 1],
        "cluster": cluster_ids,
        "label": labels
    })

    plt.figure(figsize=(8, 6))
    sns.scatterplot(
        data=df,
        x="x", y="y",
        hue="cluster",
        style=df["label"].apply(lambda v: "true" if v == 1 else ("soft" if v == 0.5 else "neg")),
        palette="tab10",
        alpha=0.7,
        s=40
    )
    plt.title("Cluster Scatter Plot")
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"âœ… Cluster æ•£é»åœ–å·²å„²å­˜è‡³: {save_path}")
    points_csv = save_path.replace(".png", ".csv")
    df.to_csv(points_csv, index=False, encoding="utf-8-sig")


def cluster_with_dataloader(args):
    """
    ç›´æ¥å¾ dataloader è®€å– npz æª”æ¡ˆè³‡æ–™é€²è¡Œ clustering
    ä¸¦å°èˆ‡ label=1 åŒç¾¤çš„æ¨£æœ¬çµ¦ soft_label=0.5
    """
    # è®€å–è³‡æ–™
    dataloader = get_dataloader(args.input_npz, batch_size=args.batch_size, shuffle=False)
    X, labels = extract_features_from_dataloader(dataloader)
    
    # print(pd.DataFrame(X).head(10))
    # æ¨™æº–åŒ–
    X_scaled = StandardScaler().fit_transform(X)

    # Clustering
    if args.method == "gmm":
        model = GaussianMixture(n_components=args.n_clusters, random_state=args.seed)
        cluster_ids = model.fit_predict(X_scaled)
    elif args.method == "kmeans":
        model = KMeans(n_clusters=args.n_clusters, random_state=args.seed)
        cluster_ids = model.fit_predict(X_scaled)
    elif args.method == "kmodes":
        model = KModes(n_clusters=args.n_clusters, init='Huang', random_state=args.seed)
        cluster_ids = model.fit_predict(X)  # ä¸éœ€ scalingï¼ŒX ç‚ºåŸå§‹é¡åˆ¥è³‡æ–™
    elif args.method == "hierarchical":
        model = AgglomerativeClustering(n_clusters=args.n_clusters, linkage="ward")
        cluster_ids = model.fit_predict(X_scaled)
    elif args.method == "dbscan":
        model = DBSCAN(eps=0.5, min_samples=5)
        cluster_ids = model.fit_predict(X_scaled)
    elif args.method == "birch":
        model = Birch(n_clusters=args.n_clusters)
        cluster_ids = model.fit_predict(X_scaled)
    elif args.method == "spectral":
        model = SpectralClustering(n_clusters=args.n_clusters, random_state=args.seed, affinity='nearest_neighbors')
        cluster_ids = model.fit_predict(X_scaled)
    else:
        raise ValueError(f"Unknown clustering method: {args.method}")

    print(f"âœ… èšé¡å®Œæˆ: n_clusters={args.n_clusters}")

    # æ‰¾å‡ºæ‰€æœ‰ label=1 çš„ç¾¤é›†
    pos_clusters = set(cluster_ids[labels == 1])
    # print(f"æœ‰æ¨™ alert çš„ç¾¤é›† ID: {pos_clusters}")

    # ç›´æ¥ä¿®æ”¹åŸå§‹ label
    new_labels = labels.astype(np.float32)
    # for i in range(len(new_labels)):
        # if cluster_ids[i] in pos_clusters and new_labels[i] == 0:
            # new_labels[i] = 0.5  # åŠæ­£æ¨£æœ¬
            
    # æ¯”ä¾‹é–¾å€¼ï¼Œå¯è‡ªè¡Œèª¿æ•´
    for c in np.unique(cluster_ids):
        cluster_mask = (cluster_ids == c)
        cluster_labels = labels[cluster_mask]
        pos_ratio = np.mean(cluster_labels == 1)
        if pos_ratio >= args.threshold:
            new_labels[cluster_mask & (labels == 0)] = args.soft_label


    unique, counts = np.unique(new_labels, return_counts=True)
    print(dict(zip(unique, counts)))
    
    # é‡æ–°è®€ npz å…§å®¹ä¸¦é™„åŠ  soft_label
    print(f"training data path: {args.output_npz}")
    

    changed = np.sum((labels != new_labels))
    print(f"å…±æœ‰ {changed} ç­†æ¨£æœ¬è¢«æ›´æ–°ç‚º label=0.5")
    
    npz_data = dict(np.load(args.input_npz, allow_pickle=True))
    npz_data["label"] = new_labels
    np.savez_compressed(args.output_npz, **npz_data)
    print("âœ… å®Œæˆ clustering ä¸¦åŠ å…¥ soft_label")
    
    plot_cluster_scatter(X_scaled, cluster_ids, new_labels, save_path=args.output_npz.replace(".npz", "_scatter_pca.png"), method="pca")
    plot_cluster_scatter(X_scaled, cluster_ids, new_labels, save_path=args.output_npz.replace(".npz", "_scatter_tsne.png"), method="tsne")



if __name__ == "__main__":
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--input_npz", required=True)
    p.add_argument("--output_npz", required=True)
    p.add_argument("--n_clusters", type=int, default=10)
    p.add_argument("--method", type=str, default="kmeans")
    p.add_argument("--batch_size", type=int, default=128)
    p.add_argument("--threshold", type=float, default=0.6)
    p.add_argument("--soft_label", type=float, default=0.5)
    p.add_argument("--seed", type=int, default=42)
    args = p.parse_args()

    cluster_with_dataloader(args=args)